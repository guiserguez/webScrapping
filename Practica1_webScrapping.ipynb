{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Practica1_webScrapping_entrega.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "LFj_7SBtxGko",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**@author Güise Rodríguez**\n",
        "\n",
        "**Execution date: 14/04/2019 21:00 **"
      ]
    },
    {
      "metadata": {
        "id": "_hLmGmf_i_pX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Instalo las librerías necesarias"
      ]
    },
    {
      "metadata": {
        "id": "Za2TOrCUvCWL",
        "colab_type": "code",
        "outputId": "c079da1e-382b-406d-fec4-fd90245bd1ba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        }
      },
      "cell_type": "code",
      "source": [
        "!pip3 install requests\n",
        "!pip3 install beautifulsoup4\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (2.18.4)\n",
            "Requirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests) (2.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests) (2019.3.9)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests) (1.22)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.6/dist-packages (4.6.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "J8I-ZQ2KkAMG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Importo las librerías necesarias"
      ]
    },
    {
      "metadata": {
        "id": "OMdBbWrfkAaE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import requests, sys, re, csv, urllib.robotparser\n",
        "from bs4 import BeautifulSoup\n",
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6xysVUTGkB3d",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# It's show time!"
      ]
    },
    {
      "metadata": {
        "id": "o3pZtd7zcZU7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Preparación para el Scraping"
      ]
    },
    {
      "metadata": {
        "id": "-dehmhQe1D90",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Modifico el user-agent para evitar bloqueos\n"
      ]
    },
    {
      "metadata": {
        "id": "PjOSm1c61Esz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "headers = {\n",
        "\"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,\\ */*;q=0.8\",\n",
        "\"Accept-Encoding\": \"gzip, deflate, sdch, br\",\n",
        "\"Accept-Language\": \"en-US,en;q=0.8\",\n",
        "\"Cache-Control\": \"no-cache\", \"dnt\": \"1\",\n",
        "\"Pragma\": \"no-cache\",\n",
        "\"Upgrade-Insecure-Requests\": \"1\",\n",
        "\"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_3) AppleWebKit/5\\ 37.36 (KHTML, like Gecko) Chrome/56.0.2924.87 Safari/537.36\" }"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "E6vEoyd61A4S",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Verifico el  fichero robots.txt"
      ]
    },
    {
      "metadata": {
        "id": "1Iw-Q5X31cFL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Para verificar que el fichero robots.txt no evita que podamos consultar las páginas con noticias que nos interesan, creamos una instancia del objeto de Python **RobotFileParser**, que con cuya función **can_fetch()** nos permite conocer devolviendo un booleano si este fichero indica o no si se puede rastrear dicho directorio."
      ]
    },
    {
      "metadata": {
        "id": "DqxocfVc1Boy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "87c6b377-8e28-4376-ae8c-89956e783a63"
      },
      "cell_type": "code",
      "source": [
        "rp = urllib.robotparser.RobotFileParser()\n",
        "rp.set_url(\"https://www.eldiario.es/robots.txt\")\n",
        "rp.read()\n",
        "rp.can_fetch(\"*\", \"https://www.eldiario.es/focos/\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "metadata": {
        "id": "TqlMtSH02aiR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 436
        },
        "outputId": "85ae9ff7-aa10-480e-acb9-ba6651b3d803"
      },
      "cell_type": "code",
      "source": [
        "request = requests.get(\"https://www.eldiario.es/robots.txt\", headers=headers)\n",
        "if request.status_code != 200:\n",
        "  sys.exit(\"Error Code: \" + str(request.status_code))\n",
        "soup = BeautifulSoup(request.content)\n",
        "print(soup.find('p').prettify())"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<p>\n",
            " User-agent: *\n",
            "Disallow: /bbtcaptcha/\n",
            "Disallow: /bbtcomment/create/\n",
            "Disallow: /bbtcomment/vote/\n",
            "Disallow: /bbtcomment/report/\n",
            "Disallow: /bbtcomment/entityComments/\n",
            "Disallow: /bbtcontent/poll/vote/\n",
            "Disallow: /bbtcontent/poll/getresults/\n",
            "Disallow: /props/\n",
            "Disallow: https://www.eldiario.es/economia/Grupo-Asfi-apropiacion-indebida-blanqueo_0_73143178.html\n",
            "Disallow: https://www.eldiario.es/norte/navarra/ultima_hora/Condenado-prision-Inaki-Gil-Asfi_0_363614222.html\n",
            "Disallow: https://www.eldiario.es/norte/navarra/ultima_hora/vale-millon-euros-carcel-llevo_0_353815044.html\n",
            "Disallow: https://www.eldiario.es/economia/juezas-pagadas-Indra-sentencias-empresa_0_359764337.html\n",
            "Disallow: https://www.eldiario.es/canariasahora/topsecret/Mckenzie-ataca-nuevo_6_97350295.html\n",
            "Disallow: /bbtmail/sendEntity/\n",
            "Disallow: /bbtshop/\n",
            "Disallow: /bbtstats/\n",
            "Disallow: /logout.html\n",
            "\n",
            "Sitemap: https://www.eldiario.es/sitemap_index.xml\n",
            "Sitemap: https://www.eldiario.es/sitemap_google_news.xml\n",
            "</p>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "A9NIcxvA1-yx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "También realizo la comprobación imprimiendo los datos del fichero robots.txt para comprobar nuevamente que la página permite rastrear el directorio seleccionado.\n",
        "\n",
        "Observamos cómo entre los directorios prohibidos a todos los robots no se encuentra **/focos**."
      ]
    },
    {
      "metadata": {
        "id": "SyqKExB01FQk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Código Principal\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "_nY9eNPwE26b",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Defino las variables que permitirán extraer la información de 5 secciones del periódico digital junto a su profundidad máxima.\n",
        "base = \"https://www.eldiario.es/focos/\"\n",
        "focos = ['mejores_ciudades','vida_digital','creacion_cultural','medio_ambiente','maltrato_animal']\n",
        "depth = [12,23,41,37,10]\n",
        "header = ['headline','type','date','section','author','authorInfo','location','url']\n",
        "csvData = []"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "w1BsJ7a1f0Aw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_author(extract):\n",
        "  \"\"\"\n",
        "  Función que busca todas las etiquetas 'a' que contienen los nombres de los \n",
        "  autores/as del contenido y las concatena separadas por comas.\n",
        "  En caso de no haber información en este campo, se adquiere la información del\n",
        "  primer contenido del extracto HTML, limpiándose los saltos de línea, guiones y \n",
        "  varios espacios seguidos que suelen tener los nombres de colectivos autores.\n",
        "  \"\"\"\n",
        "  authors = []\n",
        "  for autora in extract.find_all(\"a\"):\n",
        "    authors.append(re.sub(' +',' ',autora['title']))\n",
        "  if authors:\n",
        "    return \",\".join(map(str,authors))\n",
        "  #Si es un colectivo, el nombre aparecería con dicho formato\n",
        "  return re.sub(' +',' ',re.sub('-','',re.sub('\\n','',extract.contents[0]))).strip()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oyGc5Rt0RshI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_data(extract):\n",
        "  \"\"\"\n",
        "  Función que busca la clase asociada a los artículos de opinión\n",
        "  en las clases del código HTML y determina así si la información\n",
        "  contenida en la etiqueta 'span' con clase 'location' (si existiera)\n",
        "  es información extra sobre el autor o la localización de la noticia. \n",
        "  \"\"\"\n",
        "  extra = extract.find(\"span\",{\"class\": \"location\"})\n",
        "  if 'lst-item-opinion' in extract.get(\"class\"):\n",
        "    type = \"opinion\"\n",
        "    if extra:\n",
        "      authorInfo = extra.getText()\n",
        "      location = \"\"\n",
        "  else:\n",
        "    type = \"news\"\n",
        "    if extra:\n",
        "      authorInfo = \"\"\n",
        "      location = extra.getText()\n",
        "  if extra is None:\n",
        "    authorInfo = \"\"\n",
        "    location = \"\"\n",
        "  return type, authorInfo, location"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7vq45YVIs03h",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def save_dataset(csvData, header):\n",
        "  \"\"\"\n",
        "  Función que convierte la lista de listas csvData a Dataframe y la guarda en un csv\n",
        "  utilizando las funciones de la librería Pandas.\n",
        "  \"\"\"\n",
        "  dfObj = pd.DataFrame(csvData, columns= header)\n",
        "  dfObj.to_csv(\"eldiario_news.csv\",sep=\";\",index=False,quoting=csv.QUOTE_MINIMAL)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gfkCfo3T88yU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_page(i, foco):\n",
        "  \"\"\"\n",
        "  Función que recorre todas las páginas que tiene la sección 'foco',\n",
        "  descarga la página comprobando posibles errores en la solicitud y\n",
        "  pasa a la función 'get_noticia()' los datos contenidos en la etiqueta\n",
        "  'div' con clase 'pg-body', que contiene las noticias del medio.\n",
        "  \"\"\"\n",
        "  for nPage in np.arange(depth[i])+1:\n",
        "    target = base + foco + \"/?page=\" + str(nPage) \n",
        "    request = requests.get(target, headers=headers)\n",
        "    if request.status_code != 200:\n",
        "      sys.exit(\"Error Code: \" + str(request.status_code))\n",
        "    soup = BeautifulSoup(request.content)\n",
        "    body = soup.find(\"div\",{\"class\": \"pg-body\"})\n",
        "    get_noticia(body) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YW9kvGq08v1V",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_noticia(body):\n",
        "  \"\"\"\n",
        "  Función que recorre las diferentes noticias, buscando las diferentes\n",
        "  etiquetas 'li' con clase 'lst-item' en las que están contenidas, \n",
        "  y obtiene datos como el tipo de noticia y la información opcional\n",
        "  como información extra del autor o la localización de la noticia llamando\n",
        "  a la función 'get_data()', el título extrayendo el atributo 'title' de la\n",
        "  etiqueta 'a', la fecha de publicación de la etiqueta 'span' con clase\n",
        "  'date', la url relativa del atributo 'href' de la etiqueta 'a' y el autor\n",
        "  o autores llamando a la función 'get_author()'. Por último, concatena a la lista\n",
        "  csvData una lista con todos los datos de cada noticia.\n",
        "  \"\"\"\n",
        "  for noticia in body.find_all(\"li\",{\"class\": \"lst-item\"}):\n",
        "        type, authorInfo, location = get_data(noticia)\n",
        "        datos = noticia.find(\"span\",{\"class\": \"byline\"})\n",
        "        headline = re.sub(\"\\\\\\\\'\",\"\\'\",noticia.find(\"a\")['title']) #Sustituyo \"\\\\'\" por una comilla simple\n",
        "        date = datos.find(\"span\",{\"class\": \"date\"}).getText()\n",
        "        author = get_author(datos)\n",
        "        url = \"eldiario.es\" + noticia.find(\"a\")['href']\n",
        "        csvData.append([headline,type,date,foco,author,authorInfo,location,url])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GJAhflzlE23Q",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Código principal en el que se iteran las diferentes secciones\n",
        "'focos' y se le pasa junto a su identificador numérico para\n",
        "que la función 'get_page()' itere todas sus páginas.\n",
        "Posteriormente llama a la función 'save_dataset' para que \n",
        "guarde todas las noticias en un fichero csv.\n",
        "\"\"\"\n",
        "for i, foco in enumerate(focos):\n",
        "    get_page(i, foco)\n",
        "save_dataset(csvData, header)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}